{{- if and .Values.aggregator.enabled (not .Values.aggregator.configMap) -}}
apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ include "fluentd.fullname" . }}-aggregator-cm
  labels: {{- include "fluentd.labels" . | nindent 4 }}
    app.kubernetes.io/component: aggregator
data:
  fluentd.conf: |
    {{- if .Values.metrics.enabled -}}
    # Prometheus Exporter Plugin
    # input plugin that exports metrics
    <source>
      @type prometheus
      port {{ .Values.metrics.service.port }}
    </source>

    # input plugin that collects metrics from MonitorAgent
    <source>
      @type prometheus_monitor
      <labels>
        host ${hostname}
      </labels>
    </source>

    # input plugin that collects metrics for output plugin
    <source>
      @type prometheus_output_monitor
      <labels>
        host ${hostname}
      </labels>
    </source>
    {{- end }}

    # Ignore fluentd own events
    <match fluent.**>
      @type null
    </match>

    # TCP input to receive logs from
    {{- if and .Values.aggregator.port }}
    <source>
      @type forward
      bind 0.0.0.0
      port {{ .Values.aggregator.port }}
    </source>
    {{- end }}

    # HTTP input for the liveness and readiness probes
    <source>
      @type http
      bind 0.0.0.0
      port 9880
    </source>

    # Throw the healthcheck to the null instead of forwarding it
    <match fluentd.healthcheck>
      @type null
    </match>

    # Tag the record based on the namespace
    <match kubernetes.**>
      @type rewrite_tag_filter
      <rule>
        key $['kubernetes']['namespace_name']
        pattern ^(.+)$
        tag $1.${tag}
      </rule>
    </match>
    
    # Attempt to parse the log field as json
    <filter pension-automation-prod.kubernetes.**>
      @type parser
      key_name log
      reserve_time
      reserve_data
      remove_key_name_field true
      <parse>
        @type multi_format
        <pattern>
          format json
        </pattern>
        <pattern>
          format none
        </pattern>
      </parse>
    </filter>

    # Until PA gets their acts together and can produce valid json...
    <filter pension-automation-prod.kubernetes.**>
      @type record_modifier
      <replace>
        key message
        expression /\\/
        replace ''
      </replace>
    </filter>

    # Because consistency is hard, some events use the "mag" tag instead of "message"
    <filter pension-automation-prod.kubernetes.**>
      @type record_modifier

      <record>
        message ${record['msg']}
      </record>
      remove_keys msg
    </filter>

    # Expand out Off-ramp data
    <filter pension-automation-prod.kubernetes.**>
      @type parser
      key_name message
      reserve_time
      reserve_data
      <parse>
        @type multi_format
        <pattern>
          format regexp
          expression /^Award process completed for claim ID (?<claimid>[0-9]+)$/
        </pattern>
        # Not sure this one actually works
        <pattern>
          format regexp
          expression /^Rating process completed for claim ID (?<claimid>[0-9]+)$/
        </pattern>
        <pattern>
          format regexp
          expression /^Pension Automation Off-ramp: (?<offramp_status>[A-Z_]+) - Claim ID (?<claimid>[0-9]+) (?<offramp_reason>.*)$/
        </pattern>
        <pattern>
          format regexp
          expression /^Pension Automation Off-ramp: (?<offramp_status>[A-Z_]+) - (?<offramp_reason>.*?) for claim ID (?<claimid>[0-9]+)$/
        </pattern>
        <pattern>
          format regexp
          expression /^Claim (?<claimid>[0-9]+)/
        </pattern>
        <pattern>
          format none
        </pattern>
      </parse>
    </filter>

    # Send the logs to elasticsearch
    <match pension-automation-prod.kubernetes.**>
      @type elasticsearch_dynamic
      include_tag_key true
      scheme "#{ENV.fetch('ELASTICSEARCH_SCHEME', 'http')}"
      host "#{ENV['ELASTICSEARCH_HOST']}"
      port "#{ENV['ELASTICSEARCH_PORT']}"
      logstash_format true
      logstash_prefix ${record['kubernetes']['namespace_name']}

      <buffer>
        @type file
        path /opt/bitnami/fluentd/logs/buffers/logs.buffer
        # Flush conf
        flush_mode interval
        flush_thread_count 2
        flush_interval 5s
        # Retry conf
        retry_type exponential_backoff
        retry_forever
        retry_max_interval 30
        # Chunk conf
        total_limit_size 1024MB
        chunk_limit_size 16MB
        overflow_action block
      </buffer>
    </match>

    # Drop everything else
    <match **>
      @type null
    </match>
{{- end -}}
