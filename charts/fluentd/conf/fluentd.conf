{{- if .Values.metrics.enabled -}}
# Prometheus Exporter Plugin
# input plugin that exports metrics
<source>
  @type prometheus
  port {{ .Values.metrics.service.port }}
</source>

# input plugin that collects metrics from MonitorAgent
<source>
  @type prometheus_monitor
  <labels>
    host ${hostname}
  </labels>
</source>

# input plugin that collects metrics for output plugin
<source>
  @type prometheus_output_monitor
  <labels>
    host ${hostname}
  </labels>
</source>

# input plugin that collects metrics for in_tail plugin
<source>
  @type prometheus_tail_monitor
  <labels>
    host ${hostname}
  </labels>
</source>
{{- end }}
# Ignore fluentd events
<label @FLUENT_LOG>
  <match **>
    @type file
    path /emptydir/fluentd
  </match>
</label>

# HTTP input for the liveness and readiness probes
#<source>
#  @type http_healthcheck
#  bind 0.0.0.0
#  port 9880
#</source>

# Get the logs from the containers running on the k8s node
<source>
  @id fluentd-containers.log
  @type tail
  path /var/log/containers/*.log
  # exclude Fluentd logs
  exclude_path [
    "/var/log/containers/*fluentd*.log",
    "/var/log/containers/*flux*.log",
    "/var/log/containers/*git2consul*.log",
    "/var/log/containers/*helm*.log",
    "/var/log/containers/*tiller*.log",
    "/var/log/containers/*kiam*.log"
  ]
  pos_file /var/log/fluentd/var_log_containers.pos
  pos_file_compaction_interval 72h
  time_key time
  time_format %Y-%m-%dT%H:%M:%S.%NZ
  tag raw.kubernetes.*
  format json_in_json
  read_from_head true
</source>

# Detect Exceptions/Stacktraces
<match raw.kubernetes.**>
  @id raw.kubernetes
  @type detect_exceptions
  remove_tag_prefix raw
  stream stream
  multiline_flush_interval 5
  max_bytes 500000
  max_lines 1000
</match>

# Enrich with kubernetes metadata
<filter kubernetes.**>
  @type kubernetes_metadata
  skip_container_metadata true
</filter>

# Grab platform audit logs
<source>
  @type tail
  # audit log path of kube-apiserver
  path /var/log/kubernetes/audit/kube-apiserver-audit.log
  pos_file /var/log/fluentd/platform-audit.pos
  format json
  time_key requestReceivedTimestamp
  time_format %Y-%m-%dT%H:%M:%S.%NZ
  keep_time_key true
  tag platform-audit
</source>

# Enrich and normalize records from kube-apiserver-audit.log
<filter platform-audit>
  @id kube_api_audit_normalize
  @type record_transformer
  auto_typecast false
  enable_ruby true
  <record>
    host "#{ENV['K8S_NODE_NAME']}"
    responseObject ${record["responseObject"].nil? ? "none": record["responseObject"].to_json}
    requestObject ${record["requestObject"].nil? ? "none": record["requestObject"].to_json}
    origin kubernetes-api-audit
    time ${record["requestReceivedTimestamp"]}
  </record>
</filter>

# Grab logs from systemd-journal
<source>
  @id journald-docker
  @type systemd
  matches [{ "_SYSTEMD_UNIT": "docker.service" }]
  <storage>
    @type local
    persistent true
    path /var/log/fluentd/storage/platform-docker
  </storage>
  <entry>
    field_map {
      "MESSAGE": "message",
      "_BOOT_ID": "boot_id",
      "_GID": "gid",
      "_HOSTNAME": "hostname",
      "_PID": "pid",
      "_SELINUX_CONTEXT": "selinux_context",
      "_TRANSPORT": "transport",
      "_UID": "uid"
    }
    field_map_strict true
  </entry>
  read_from_head true
  tag platform-docker
</source>

<source>
  @id journald-kubelet
  @type systemd
  matches [{ "_SYSTEMD_UNIT": "kubelet.service" }]
  <storage>
    @type local
    persistent true
    path /var/log/fluentd/storage/platform-kubelet
  </storage>
  <entry>
    field_map {
      "MESSAGE": "message",
      "_BOOT_ID": "boot_id",
      "_GID": "gid",
      "_HOSTNAME": "hostname",
      "_PID": "pid",
      "_SELINUX_CONTEXT": "selinux_context",
      "_TRANSPORT": "transport",
      "_UID": "uid"
    }
    field_map_strict true
  </entry>
  read_from_head true
  tag platform-kubelet
</source>

# Break out bip-app and bip-env fields based on namespace
<filter kubernetes.**>
  @type parser
  key_name $.kubernetes.namespace_name
  reserve_time
  reserve_data
  <parse>
    @type multi_format
    <pattern>
      format regexp
      expression /^(?<bip-app>[a-zA-Z\-]+?)-(?<bip-env>({{ join "|" .Values.environments }}))$/
    </pattern>
    <pattern>
      format regexp
      expression /^(?<bip-app>.*)$/
    </pattern>
  </parse>
</filter>

## Generate product-line field
#<filter kubernetes.**>
#  @type record_modifier
#  <record>
#    bip-product-line ${record.dig('kubernetes', 'namespace_labels', 'bip_va_gov/product-line') || 'orphan'}
#  </record>
#</filter>

# Tag the record based on the value of bip-app
<match kubernetes.**>
  @type rewrite_tag_filter
  <rule>
    key bip-app
    pattern ^(.+)$
    tag $1.${tag}
  </rule>
</match>

# Drop all consul health check alerts because it's loud and worthless.
<filter vault.kubernetes.**>
  @type grep
  <exclude>
    key log
    pattern /Check .* HTTP request failed/
  </exclude>
</filter>

# Attempt to parse the log field as json
#<filter *.kubernetes.**>
#  @type parser
#  key_name log
#  reserve_time
#  reserve_data
#  remove_key_name_field true
#  <parse>
#    @type multi_format
#    <pattern>
#      format json
#      types claimid:integer
#    </pattern>
#    <pattern>
#      format none
#    </pattern>
#  </parse>
#</filter>

## Rename variations to "message" field for consistency
#<filter kubernetes.**>
#  @type rename_key
#  rename_rule1 ^msg$ message
#  #rename_rule2 ^MESSAGE$ message
#</filter>

# Generate a hash id for each record to prevent duplicates in ES.
# see https://github.com/uken/fluent_plugin-elasticsearch#generate-hash-id
<filter **>
  @type elasticsearch_genid
  hash_id_key _hash
</filter>

<filter platform-*>
  @type record_modifier
  <record>
    _target_index ${tag}-${Time.at(time).strftime('%F')}
  </record>
</filter>

# Set the index dynamically based on k8s metadata
<filter *.kubernetes.**>
  @type record_modifier
  <record>
    _target_index ns-${record['bip-app']}-${Time.at(time).strftime('%F')}
  </record>
</filter>

# Set any static fields
<filter **>
  @type record_transformer
  <record>
    {{ range $k, $v := .Values.fields }}
      {{ $k }} {{ $v | quote }}
    {{ end }}
  </record>
</filter>

{{ if .Values.aggregator.enabled }}
# Forward all logs to the aggregators
<match **>
  @type forward
  {{- $fullName := (include "fluentd.fullname" .) }}
  {{- $global := . }}
  {{- $domain := default "cluster.local" .Values.clusterDomain }}
  {{- $port := .Values.aggregator.port | int }}
  {{- range $i, $e := until (.Values.aggregator.replicaCount | int) }}
  <server>
    {{ printf "host %s-%d.%s-headless.%s.svc.%s" $fullName $i $fullName $global.Release.Namespace $domain }}
    {{ printf "port %d" $port }}
    {{- if ne $i 0 }}
    standby
    {{- end }}
  </server>
  {{- end}}

  <buffer>
    @type file
    path /var/log/fluentd/buffers/forwarding
    # Retry
    retry_type exponential_backoff
    retry_forever
    retry_max_interval 30
    # Flush
    flush_mode interval
    flush_thread_count 2
    flush_interval 5s
    total_limit_size 1024MB
    chunk_limit_size 25MB
    overflow_action block
  </buffer>
</match>

# Drop everything else - only here to catch events when filtering out the above match
<match **>
  @type null
</match>
{{- else }}
## Send the logs to opensearch
<match **>
  @type copy
  copy_mode shallow
  {{- if .Values.opensearch.enabled }}
  <store>
    @type opensearch
    # Use generated hash for _id to prevent duplicates in ES
    id_key _hash
    # ES does not like keys with _, so remove from record
    remove_keys _hash, time
    # Use the generate index name
    target_index_key _target_index
    #templates {
    #  "tenant-bia": "/opt/bitnami/fluentd/conf/tenant-bia.json",
    #  "tenant-candp": "/opt/bitnami/fluentd/conf/tenant-candp.json",
    #  "tenant-evre": "/opt/bitnami/fluentd/conf/tenant-evre.json",
    #  "tenant-mbs": "/opt/bitnami/fluentd/conf/tenant-mbs.json",
    #  "tenant-platform": "/opt/bitnami/fluentd/conf/tenant-platform.json"
    #}
    include_tag_key true
    include_timestamp true
    # Errors are not descriptive enough without this
    log_os_400_reason true
    #ignore_exceptions ["Opensearch::Transport::Transport::Errors::BadRequest"]
    exception_backup false
    <endpoint>
      url {{ required ".Values.opensearch.url is required since opensearch.enabled == true." .Values.opensearch.url }}
      region {{ required ".Values.aws.region is required." .Values.aws.region }}
      {{- if .Values.opensearch.assume_role.enabled }}
      assume_role_arn {{ required ".Values.opensearch.assume_role.arn must be set if .Values.opensearch.assum_role.enabled" .Values.opensearch.assume_role.arn }}
      sts_credentials_region {{ .Values.aws.region }}
      {{- end }}
    </endpoint>
    request_timeout 90s
    # Reload on errors but not based on number of events
    reload_connections false
    reload_on_failure true
    reconnect_on_error true
    <buffer>
      @type file
      path /var/log/fluentd/buffers/opensearch
      # Flush conf
      flush_mode interval
      flush_thread_count 8
      flush_interval 30s
      # Retry conf
      retry_type exponential_backoff
      retry_forever
      retry_max_interval 30
      # Chunk conf
      total_limit_size 1024MB
      chunk_limit_size 25MB
      overflow_action block
    </buffer>
  </store>
  {{- end -}}
  {{- if .Values.s3.enabled }}
  <store>
    @type s3
    s3_bucket {{ required ".Values.s3.bucket is required since s3.enabled == true." .Values.s3.bucket }}
    s3_region {{ required ".Values.aws.region is required since s3.enabled == true." .Values.aws.region }}
    #path ${$._target_index}/%Y/%m/
    path %Y/%m/%d/
    s3_object_key_format %{path}%{time_slice}_%{uuid_flush}_%{index}.%{file_extension}
    time_slice_format %Y-%m-%d.%H%M
    <buffer time>
      @type file
      path /var/log/fluentd/buffers/s3
      timekey 600
      timekey_wait 1m
      # Retry conf
      retry_type exponential_backoff
      retry_forever
      retry_max_interval 30
      # Chunk conf
      total_limit_size 2048MB
      chunk_limit_size 25MB
      overflow_action drop_oldest_chunk
    </buffer>
  </store>
  {{ end }}
</match>
{{- end }}
